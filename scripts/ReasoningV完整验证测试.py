#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ReasoningVä¼˜åŒ–åå®Œæ•´éªŒè¯æµ‹è¯•è„šæœ¬
ä½¿ç”¨æ‰€æœ‰ä¼˜åŒ–åçš„ç­–ç•¥é…ç½®ï¼Œå®Œæ•´æµ‹è¯•AMSBenchæ‰€æœ‰é¢˜ç›®
ç¡®ä¿å‡†ç¡®ç‡å¯é‡å¤
"""

import json
import torch
import time
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Any, Tuple
import warnings
warnings.filterwarnings("ignore")

class ReasoningVFullValidation:
    """ReasoningVå®Œæ•´éªŒè¯æµ‹è¯•å™¨"""
    
    def __init__(self, model_path: str):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        
        # ä»»åŠ¡é…ç½®ï¼ˆä½¿ç”¨å®é™…çš„æ•°æ®è·¯å¾„ï¼‰
        self.tasks = {
            "LDO Task": {
                "data_dir": "reasoning_task/LDO/LDO_QA/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "Comparator Task": {
                "data_dir": "reasoning_task/Comparator/comparator_QA/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "Bandgap Task": {
                "data_dir": "reasoning_task/Bandgap/bandgap_QA/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "TQA Task": {
                "data_file": "TQA Task/TQA Task.json",
                "groundtruth_field": "groundtruth"
            },
            "Caption Task": {
                "data_dir": "Caption_task/test_QA_caption/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "Opamp Task": {
                "data_dir": "reasoning_task/Opamp/test_QA_opamp/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            }
        }
        
        # åŠ è½½ä¼˜åŒ–åçš„ç­–ç•¥é…ç½®
        self.optimized_configs = self.load_optimized_configs()
        
        print(f"ğŸš€ åˆå§‹åŒ–ReasoningVå®Œæ•´éªŒè¯æµ‹è¯•å™¨")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   ä»»åŠ¡æ•°: {len(self.tasks)}")
        print(f"   å·²åŠ è½½ä¼˜åŒ–é…ç½®: {len(self.optimized_configs)} ä¸ªä»»åŠ¡")
    
    def load_optimized_configs(self) -> Dict[str, Dict]:
        """åŠ è½½æ‰€æœ‰ä¼˜åŒ–åçš„ç­–ç•¥é…ç½®"""
        configs = {}
        
        # ä»»åŠ¡åç§°æ˜ å°„ï¼šFew-shoté…ç½®ä¸­çš„åç§° -> å®é™…ä»»åŠ¡åç§°
        task_name_map = {
            "LDO": "LDO Task",
            "Comparator": "Comparator Task",
            "Caption": "Caption Task",
            "Bandgap": "Bandgap Task",
            "Opamp": "Opamp Task",
            "TQA": "TQA Task"
        }
        
        # ä¼˜å…ˆåŠ è½½æœ€æ–°ä¼˜åŒ–ç»“æœï¼ˆLDO, Comparator, Captionï¼‰
        try:
            with open("reasoningv_latest_optimization_results.json", 'r') as f:
                data = json.load(f)
                if 'optimization_results' in data:
                    results = data['optimization_results']
                    for task_key, task_result in results.items():
                        if 'strategy' in task_result and task_key in task_name_map:
                            task_name = task_name_map[task_key]
                            strategy = task_result['strategy']
                            
                            # è½¬æ¢é…ç½®æ ¼å¼
                            config = {
                                "expert_instruction": strategy.get("expert_instruction", ""),
                                "params": strategy.get("params", {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                                                  "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}),
                                "use_few_shot": strategy.get("use_few_shot", False),
                                "num_examples": strategy.get("num_examples", 2)
                            }
                            
                            configs[task_name] = config
                            print(f"   âœ… åŠ è½½ {task_name} æœ€æ–°ä¼˜åŒ–é…ç½® (Few-shot: {config['use_few_shot']}, ç¤ºä¾‹æ•°: {config.get('num_examples', 0)})")
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½æœ€æ–°ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")
            # å›é€€åˆ°Few-shotä¼˜åŒ–ç»“æœ
            try:
                with open("reasoningv_fewshot_optimization_results.json", 'r') as f:
                    data = json.load(f)
                    if 'optimization_results' in data:
                        results = data['optimization_results']
                        for task_key, task_result in results.items():
                            if 'best_strategy' in task_result and task_key in task_name_map:
                                task_name = task_name_map[task_key]
                                strategy = task_result['best_strategy']
                                
                                # è½¬æ¢é…ç½®æ ¼å¼
                                config = {
                                    "prompt": strategy.get("prompt_template", "Question: {question}\n\nOptions:\n{options}\n\nAnswer:"),
                                    "params": strategy.get("parameters", {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                                                          "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}),
                                    "use_few_shot": strategy.get("use_few_shot", False)
                                }
                                
                                # åŠ è½½Few-shotç¤ºä¾‹
                                if config["use_few_shot"]:
                                    config["few_shot_examples"] = self.load_few_shot_examples(task_name)
                                
                                configs[task_name] = config
                                print(f"   âœ… åŠ è½½ {task_name} Few-shoté…ç½®")
            except Exception as e2:
                print(f"   âš ï¸ åŠ è½½Few-shoté…ç½®å¤±è´¥: {e2}")
        
        # åŠ è½½TQAé”™è¯¯æ¨¡å¼ä¼˜åŒ–ç»“æœ
        try:
            with open("reasoningv_tqa_pattern_optimization_results.json", 'r') as f:
                data = json.load(f)
                if 'optimization_results' in data and 'result' in data['optimization_results']:
                    result = data['optimization_results']['result']
                    if 'strategy_map' in result:
                        # è½¬æ¢strategy_mapçš„é”®ä¸ºæ•´æ•°
                        strategy_map = {}
                        for k, v in result['strategy_map'].items():
                            try:
                                strategy_map[int(k)] = v
                            except:
                                pass
                        
                        configs["TQA Task"] = {
                            'type': 'pattern_optimized',
                            'strategy_map': strategy_map,
                            'base_strategy': {
                                "prompt": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                                "params": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                          "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}
                            }
                        }
                        print(f"   âœ… åŠ è½½ TQA Task é”™è¯¯æ¨¡å¼ä¼˜åŒ–é…ç½® ({len(strategy_map)} ä¸ªç‰¹æ®Šç­–ç•¥)")
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½TQAé…ç½®å¤±è´¥: {e}")
        
        # åŠ è½½å…¶ä»–ä»»åŠ¡çš„ä¼˜åŒ–é…ç½®ï¼ˆBandgapå’ŒOpampï¼‰
        try:
            with open("reasoningv_fewshot_optimization_results.json", 'r') as f:
                data = json.load(f)
                if 'optimization_results' in data:
                    results = data['optimization_results']
                    for task_key in ["Bandgap", "Opamp"]:
                        if task_key in results and task_name_map[task_key] not in configs:
                            task_name = task_name_map[task_key]
                            task_result = results[task_key]
                            if 'best_strategy' in task_result:
                                strategy = task_result['best_strategy']
                                configs[task_name] = {
                                    "prompt": strategy.get("prompt_template", "Question: {question}\n\nOptions:\n{options}\n\nAnswer:"),
                                    "params": strategy.get("parameters", {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                                                          "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True})
                                }
                                print(f"   âœ… åŠ è½½ {task_name} ä¼˜åŒ–é…ç½®")
        except:
            pass
        
        # ä¸ºæœªé…ç½®çš„ä»»åŠ¡è®¾ç½®é»˜è®¤é…ç½®
        for task_name in self.tasks.keys():
            if task_name not in configs:
                configs[task_name] = {
                    "prompt": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                    "params": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                              "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}
                }
                print(f"   âš ï¸ {task_name} ä½¿ç”¨é»˜è®¤é…ç½®")
        
        return configs
    
    def load_few_shot_examples(self, task_name: str, num_examples: int = 2) -> List[Dict]:
        """åŠ è½½Few-shotç¤ºä¾‹ï¼ˆæ¯æ¬¡è°ƒç”¨éšæœºé€‰æ‹©ï¼Œæ¨¡æ‹Ÿä¼˜åŒ–æ—¶çš„éšæœºæ€§ï¼‰"""
        import random
        # ä¸è®¾ç½®å›ºå®šç§å­ï¼Œè®©æ¯æ¬¡è°ƒç”¨éƒ½éšæœºé€‰æ‹©ï¼ˆæ¨¡æ‹Ÿä¼˜åŒ–æ—¶çš„è¡Œä¸ºï¼‰
        # è¿™æ ·å¤šæ¬¡è¿è¡Œå–å¹³å‡æ‰èƒ½å¾—åˆ°ç¨³å®šç»“æœ
        
        questions = self.load_task_data(task_name)
        if not questions:
            return []
        
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        
        # é€‰æ‹©æœ‰æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬ä½œä¸ºç¤ºä¾‹ï¼ˆä¸ä¼˜åŒ–è„šæœ¬ä¸€è‡´ï¼‰
        valid_examples = [
            q for q in questions 
            if q.get('question') and q.get('options') and q.get(groundtruth_field)
        ]
        
        if len(valid_examples) < num_examples:
            return valid_examples
        
        # éšæœºé€‰æ‹©ï¼ˆä¸è®¾ç½®ç§å­ï¼Œæ¯æ¬¡è°ƒç”¨éƒ½ä¸åŒï¼‰
        return random.sample(valid_examples, min(num_examples, len(valid_examples)))
    
    def build_few_shot_prompt(self, task_name: str, question: str, options: Dict[str, str], 
                              examples: List[Dict], expert_instruction: str = "") -> str:
        """æ„å»ºFew-shotæç¤ºè¯ï¼ˆä¸ä¼˜åŒ–è„šæœ¬ä¸€è‡´ï¼‰"""
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        
        prompt_parts = []
        
        # æ·»åŠ ä»»åŠ¡ç‰¹å®šçš„æŒ‡å¯¼ï¼ˆä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„expert_instructionï¼‰
        if expert_instruction:
            prompt_parts.append(expert_instruction)
        else:
            # å›é€€åˆ°é»˜è®¤æŒ‡å¯¼
            if task_name == "LDO Task":
                prompt_parts.append("You are an LDO circuit expert. Analyze LDO circuits by checking:\n1. Pass transistor (source fixed at VDD)\n2. Error amplifier (compares VREF with feedback)\n3. Stable bandgap reference\n4. Resistive divider feedback network\n")
            elif task_name == "Comparator Task":
                prompt_parts.append("You are a comparator circuit expert. Analyze comparator circuits systematically.\n")
            elif task_name == "Caption Task":
                prompt_parts.append("You are a caption analysis expert. Evaluate all options equally.\n")
        
        # æ·»åŠ few-shotç¤ºä¾‹ï¼ˆä½¿ç”¨æ‰€æœ‰æä¾›çš„ç¤ºä¾‹ï¼‰
        if examples:
            prompt_parts.append("Examples:\n")
            for i, example in enumerate(examples, 1):
                ex_question = example.get('question', '')
                ex_options = example.get('options', {})
                ex_answer = example.get(groundtruth_field, '')
                
                if ex_question and ex_options and ex_answer:
                    options_str = ""
                    for key, value in ex_options.items():
                        # ç®€åŒ–é€‰é¡¹æ˜¾ç¤ºï¼ˆä¸ä¼˜åŒ–è„šæœ¬ä¸€è‡´ï¼‰
                        value_short = value[:200] + "..." if len(value) > 200 else value
                        options_str += f"{key}. {value_short}\n"
                    
                    prompt_parts.append(f"Example {i}:\nQuestion: {ex_question}\nOptions:\n{options_str}Answer: {ex_answer}\n\n")
        
        # æ·»åŠ å½“å‰é—®é¢˜ï¼ˆå§‹ç»ˆä½¿ç”¨"Now solve this:"æ ¼å¼ï¼Œä¸ä¼˜åŒ–è„šæœ¬ä¸€è‡´ï¼‰
        options_str = ""
        for key, value in options.items():
            options_str += f"{key}. {value}\n"
        
        prompt_parts.append(f"Now solve this:\nQuestion: {question}\nOptions:\n{options_str}Answer:")
        
        return "\n".join(prompt_parts)
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is not None:
            return
        
        print(f"\nğŸ“¥ æ­£åœ¨åŠ è½½ReasoningVæ¨¡å‹...")
        import sys
        sys.stdout.flush()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map={"": 0},
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        print(f"âœ… ReasoningVæ¨¡å‹åŠ è½½å®Œæˆ")
        import sys
        sys.stdout.flush()
    
    def load_task_data(self, task_name: str) -> List[Dict]:
        """åŠ è½½ä»»åŠ¡æ•°æ®"""
        if task_name not in self.tasks:
            return []
        
        task_config = self.tasks[task_name]
        
        # å•ä¸ªæ–‡ä»¶æ¨¡å¼ï¼ˆTQAï¼‰
        if "data_file" in task_config:
            data_file = task_config["data_file"]
            if not os.path.exists(data_file):
                return []
            
            try:
                with open(data_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
                return []
        else:
            # ç›®å½•æ¨¡å¼ï¼ˆå…¶ä»–ä»»åŠ¡ï¼‰
            import glob
            data_dir = task_config["data_dir"]
            file_pattern = task_config["file_pattern"]
            
            if not os.path.exists(data_dir):
                return []
            
            all_data = []
            try:
                files = glob.glob(os.path.join(data_dir, file_pattern))
                for file_path in files:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        file_data = json.load(f)
                        if isinstance(file_data, list):
                            all_data.extend(file_data)
                        else:
                            all_data.append(file_data)
                return all_data
            except Exception as e:
                print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
                return []
    
    def build_prompt(self, template: str, question: str, options: Dict[str, str], 
                    few_shot_examples: List[Dict] = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        options_str = ""
        if options:
            for key, value in options.items():
                options_str += f"{key}. {value}\n"
        
        # æ·»åŠ Few-shotç¤ºä¾‹
        few_shot_text = ""
        if few_shot_examples:
            few_shot_text = "\n\nExamples:\n"
            for ex in few_shot_examples:
                ex_options = ""
                for k, v in ex.get('options', {}).items():
                    ex_options += f"{k}. {v}\n"
                few_shot_text += f"Q: {ex['question']}\nOptions:\n{ex_options}Answer: {ex['groundtruth']}\n\n"
        
        return template.format(question=question, options=options_str.strip(), 
                              few_shot_examples=few_shot_text)
    
    def generate_answer(self, prompt: str, parameters: Dict) -> Tuple[str, float]:
        """ç”Ÿæˆç­”æ¡ˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        model_device = next(self.model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=parameters.get("max_new_tokens", 1),
                temperature=parameters.get("temperature", 0.0),
                do_sample=parameters.get("do_sample", False),
                repetition_penalty=parameters.get("repetition_penalty", 1.0),
                top_p=parameters.get("top_p", 1.0),
                top_k=parameters.get("top_k", 1),
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=parameters.get("use_cache", True)
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer_part = response[len(prompt):].strip()
        
        for option in ['A', 'B', 'C', 'D', 'E']:
            if option in answer_part:
                return option, 0.95
        
        return 'A', 0.95
    
    def test_task(self, task_name: str, num_runs: int = 1) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªä»»åŠ¡ï¼ˆæ”¯æŒå¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼Œä¸ä¼˜åŒ–æ—¶ä¸€è‡´ï¼‰"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æµ‹è¯•ä»»åŠ¡: {task_name}")
        if num_runs > 1:
            print(f"   å°†è¿è¡Œ {num_runs} æ¬¡å–å¹³å‡å€¼ï¼ˆä¸ä¼˜åŒ–æ—¶ä¸€è‡´ï¼‰")
        print(f"{'='*80}")
        import sys
        sys.stdout.flush()
        
        questions = self.load_task_data(task_name)
        if not questions:
            print(f"âŒ æ— æ³•åŠ è½½ {task_name} æ•°æ®")
            return None
        
        print(f"   åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
        import sys
        sys.stdout.flush()
        
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        config = self.optimized_configs.get(task_name, {})
        
        # å¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼ˆFew-shotç¤ºä¾‹æ˜¯éšæœºçš„ï¼‰
        all_accuracies = []
        all_correct_counts = []
        all_total_times = []
        
        for run in range(num_runs):
            if num_runs > 1:
                print(f"   è¿è¡Œ {run+1}/{num_runs}...")
                import sys
                sys.stdout.flush()
            
            correct_count = 0
            total_time = 0
            start_time = time.time()
            
            # å¤„ç†Few-shoté…ç½®ï¼ˆæ¯æ¬¡è¿è¡Œé‡æ–°é€‰æ‹©ç¤ºä¾‹ï¼‰
            few_shot_examples = None
            num_few_shot = config.get('num_examples', 2)  # ä»é…ç½®ä¸­è¯»å–ç¤ºä¾‹æ•°é‡
            if config.get('use_few_shot', False):
                # æ¯æ¬¡è¿è¡Œé‡æ–°é€‰æ‹©Few-shotç¤ºä¾‹ï¼ˆæ¨¡æ‹Ÿä¼˜åŒ–æ—¶çš„éšæœºæ€§ï¼‰
                few_shot_examples = self.load_few_shot_examples(task_name, num_examples=num_few_shot)
            
            # å¤„ç†TQAé”™è¯¯æ¨¡å¼ä¼˜åŒ–é…ç½®
            error_indices = []  # è®°å½•é”™è¯¯é¢˜ç›®çš„ç´¢å¼•ï¼ˆä»…ç”¨äºTQAä»»åŠ¡ï¼‰
            if isinstance(config, dict) and config.get('type') == 'pattern_optimized':
                strategy_map = config.get('strategy_map', {})
                base_strategy = config.get('base_strategy', {
                    "prompt": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                    "params": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                              "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}
                })
                
                # ä½¿ç”¨å¤šç­–ç•¥æ˜ å°„
                for i, question_data in enumerate(questions):
                    question = question_data.get('question', '')
                    options = question_data.get('options', {})
                    groundtruth = question_data.get(groundtruth_field, '')
                    
                    if not question or not groundtruth:
                        continue
                    
                    try:
                        # æ ¹æ®é¢˜ç›®ç´¢å¼•é€‰æ‹©ç­–ç•¥
                        if i in strategy_map:
                            strategy = strategy_map[i]
                        else:
                            strategy = base_strategy
                        
                        prompt = self.build_prompt(strategy["prompt"], question, options)
                        q_start_time = time.time()
                        answer, _ = self.generate_answer(prompt, strategy["params"])
                        elapsed_time = time.time() - q_start_time
                        total_time += elapsed_time
                        
                        if answer == groundtruth:
                            correct_count += 1
                        else:
                            # è®°å½•é”™è¯¯é¢˜ç›®ç´¢å¼•ï¼ˆä»…ç”¨äºTQAä»»åŠ¡ï¼‰
                            if task_name == "TQA Task":
                                error_indices.append(i)
                    except Exception as e:
                        if task_name == "TQA Task":
                            error_indices.append(i)
                        continue
            else:
                # æ ‡å‡†é…ç½®æˆ–Few-shoté…ç½®
                prompt_template = config.get("prompt", "Question: {question}\n\nOptions:\n{options}\n\nAnswer:")
                params = config.get("params", {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                             "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True})
                
                for i, question_data in enumerate(questions):
                    question = question_data.get('question', '')
                    options = question_data.get('options', {})
                    groundtruth = question_data.get(groundtruth_field, '')
                    
                    if not question or not groundtruth:
                        continue
                    
                    try:
                        # æ„å»ºæç¤ºè¯ï¼ˆFew-shotæˆ–æ ‡å‡†ï¼‰
                        if config.get('use_few_shot', False) and few_shot_examples:
                            expert_instruction = config.get('expert_instruction', '')
                            prompt = self.build_few_shot_prompt(task_name, question, options, 
                                                               few_shot_examples, expert_instruction)
                        else:
                            prompt = self.build_prompt(prompt_template, question, options)
                        
                        q_start_time = time.time()
                        answer, _ = self.generate_answer(prompt, params)
                        elapsed_time = time.time() - q_start_time
                        total_time += elapsed_time
                        
                        if answer == groundtruth:
                            correct_count += 1
                        else:
                            # è®°å½•é”™è¯¯é¢˜ç›®ç´¢å¼•ï¼ˆä»…ç”¨äºTQAä»»åŠ¡ï¼‰
                            if task_name == "TQA Task":
                                error_indices.append(i)
                    except Exception as e:
                        if task_name == "TQA Task":
                            error_indices.append(i)
                        continue
            
            elapsed_total = time.time() - start_time
            accuracy = correct_count / len(questions) * 100 if questions else 0
            
            all_accuracies.append(accuracy)
            all_correct_counts.append(correct_count)
            all_total_times.append(elapsed_total)
            
            if num_runs > 1:
                print(f"      è¿è¡Œ {run+1} å‡†ç¡®ç‡: {accuracy:.2f}%")
                import sys
                sys.stdout.flush()
        
        # è®¡ç®—å¹³å‡å€¼
        avg_accuracy = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0
        avg_correct_count = sum(all_correct_counts) / len(all_correct_counts) if all_correct_counts else 0
        avg_total_time = sum(all_total_times) / len(all_total_times) if all_total_times else 0
        
        result = {
            'task_name': task_name,
            'accuracy': avg_accuracy,
            'correct_count': int(round(avg_correct_count)),
            'total_questions': len(questions),
            'avg_time': avg_total_time / len(questions) if questions else 0,
            'total_time': avg_total_time,
            'num_runs': num_runs,
            'individual_accuracies': all_accuracies if num_runs > 1 else None
        }
        
        # å¦‚æœæ˜¯TQAä»»åŠ¡ï¼Œç»Ÿè®¡é”™è¯¯éš¾åº¦åˆ†å¸ƒ
        if task_name == "TQA Task" and 'error_indices' in locals():
            error_stats = self.analyze_tqa_errors_by_difficulty(questions, error_indices)
            result['error_difficulty_distribution'] = error_stats
            print(f"\n   ğŸ“Š TQAé”™è¯¯éš¾åº¦åˆ†å¸ƒ:")
            for level, stats in error_stats['error_stats'].items():
                print(f"      {level}: {stats['errors']}é¢˜é”™è¯¯ ({stats['error_rate']:.2f}%é”™è¯¯ç‡, {stats['accuracy']:.2f}%å‡†ç¡®ç‡)")
            import sys
            sys.stdout.flush()
        
        print(f"\n   âœ… {task_name} æµ‹è¯•å®Œæˆ")
        print(f"      å‡†ç¡®ç‡: {avg_accuracy:.2f}% ({'å¹³å‡' if num_runs > 1 else ''})")
        print(f"      æ­£ç¡®æ•°: {result['correct_count']}/{len(questions)}")
        if num_runs > 1:
            print(f"      å„æ¬¡è¿è¡Œ: {[f'{a:.2f}%' for a in all_accuracies]}")
        print(f"      æ€»æ—¶é—´: {avg_total_time:.1f}ç§’")
        import sys
        sys.stdout.flush()
        
        return result
    
    def analyze_tqa_errors_by_difficulty(self, questions: List[Dict], error_indices: List[int]) -> Dict[str, Any]:
        """åˆ†æTQAé”™è¯¯åœ¨å„éš¾åº¦çº§åˆ«çš„åˆ†å¸ƒ"""
        from collections import Counter
        
        error_by_level = Counter()
        total_by_level = Counter()
        
        for i, question in enumerate(questions):
            level = question.get('level', 'Unknown')
            total_by_level[level] += 1
            
            if i in error_indices:
                error_by_level[level] += 1
        
        error_stats = {}
        for level in sorted(total_by_level.keys()):
            total = total_by_level[level]
            errors_count = error_by_level[level]
            error_rate = errors_count / total * 100 if total > 0 else 0
            accuracy = (total - errors_count) / total * 100 if total > 0 else 0
            
            error_stats[level] = {
                'total_questions': total,
                'errors': errors_count,
                'correct': total - errors_count,
                'error_rate': error_rate,
                'accuracy': accuracy
            }
        
        return {
            'error_stats': error_stats,
            'total_errors': len(error_indices),
            'total_questions': len(questions)
        }
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æµ‹è¯•ä»»åŠ¡: {task_name}")
        print(f"{'='*80}")
        import sys
        sys.stdout.flush()
        
        questions = self.load_task_data(task_name)
        if not questions:
            print(f"âŒ æ— æ³•åŠ è½½ {task_name} æ•°æ®")
            return None
        
        print(f"   åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
        import sys
        sys.stdout.flush()
        
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        config = self.optimized_configs.get(task_name, {})
        
        correct_count = 0
        total_time = 0
        start_time = time.time()
        
        # å¤„ç†Few-shoté…ç½®
        few_shot_examples = None
        num_few_shot = config.get('num_examples', 2)  # ä»é…ç½®ä¸­è¯»å–ç¤ºä¾‹æ•°é‡
        if config.get('use_few_shot', False):
            few_shot_examples = config.get('few_shot_examples', [])
            if not few_shot_examples:
                # å¦‚æœæ²¡æœ‰é¢„åŠ è½½ï¼Œç°åœ¨åŠ è½½
                few_shot_examples = self.load_few_shot_examples(task_name, num_examples=num_few_shot)
        
        # å¤„ç†TQAé”™è¯¯æ¨¡å¼ä¼˜åŒ–é…ç½®
        if isinstance(config, dict) and config.get('type') == 'pattern_optimized':
            strategy_map = config.get('strategy_map', {})
            base_strategy = config.get('base_strategy', {
                "prompt": "Question: {question}\n\nOptions:\n{options}\n\nAnswer:",
                "params": {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                          "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True}
            })
            
            # ä½¿ç”¨å¤šç­–ç•¥æ˜ å°„
            for i, question_data in enumerate(questions):
                question = question_data.get('question', '')
                options = question_data.get('options', {})
                groundtruth = question_data.get(groundtruth_field, '')
                
                if not question or not groundtruth:
                    continue
                
                try:
                    # æ ¹æ®é¢˜ç›®ç´¢å¼•é€‰æ‹©ç­–ç•¥
                    if i in strategy_map:
                        strategy = strategy_map[i]
                    else:
                        strategy = base_strategy
                    
                    prompt = self.build_prompt(strategy["prompt"], question, options)
                    q_start_time = time.time()
                    answer, _ = self.generate_answer(prompt, strategy["params"])
                    elapsed_time = time.time() - q_start_time
                    total_time += elapsed_time
                    
                    if answer == groundtruth:
                        correct_count += 1
                    
                    # æ¯50é¢˜è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 50 == 0:
                        print(f"      è¿›åº¦: {i+1}/{len(questions)}, å½“å‰å‡†ç¡®ç‡: {correct_count/(i+1)*100:.2f}%")
                        import sys
                        sys.stdout.flush()
                except Exception as e:
                    continue
        else:
            # æ ‡å‡†é…ç½®æˆ–Few-shoté…ç½®
            prompt_template = config.get("prompt", "Question: {question}\n\nOptions:\n{options}\n\nAnswer:")
            params = config.get("params", {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False,
                                         "repetition_penalty": 1.0, "top_p": 1.0, "top_k": 1, "use_cache": True})
            
            for i, question_data in enumerate(questions):
                question = question_data.get('question', '')
                options = question_data.get('options', {})
                groundtruth = question_data.get(groundtruth_field, '')
                
                if not question or not groundtruth:
                    continue
                
                try:
                    # æ„å»ºæç¤ºè¯ï¼ˆFew-shotæˆ–æ ‡å‡†ï¼‰
                    if config.get('use_few_shot', False) and few_shot_examples:
                        expert_instruction = config.get('expert_instruction', '')
                        prompt = self.build_few_shot_prompt(task_name, question, options, 
                                                           few_shot_examples, expert_instruction)
                    else:
                        prompt = self.build_prompt(prompt_template, question, options)
                    
                    q_start_time = time.time()
                    answer, _ = self.generate_answer(prompt, params)
                    elapsed_time = time.time() - q_start_time
                    total_time += elapsed_time
                    
                    if answer == groundtruth:
                        correct_count += 1
                    
                    # æ¯50é¢˜è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 50 == 0:
                        print(f"      è¿›åº¦: {i+1}/{len(questions)}, å½“å‰å‡†ç¡®ç‡: {correct_count/(i+1)*100:.2f}%")
                        import sys
                        sys.stdout.flush()
                except Exception as e:
                    continue
        
        elapsed_total = time.time() - start_time
        accuracy = correct_count / len(questions) * 100 if questions else 0
        avg_time = total_time / len(questions) if questions else 0
        
        result = {
            'task_name': task_name,
            'accuracy': accuracy,
            'correct_count': correct_count,
            'total_questions': len(questions),
            'avg_time': avg_time,
            'total_time': elapsed_total
        }
        
        print(f"\n   âœ… {task_name} æµ‹è¯•å®Œæˆ")
        print(f"      å‡†ç¡®ç‡: {accuracy:.2f}%")
        print(f"      æ­£ç¡®æ•°: {correct_count}/{len(questions)}")
        print(f"      å¹³å‡æ—¶é—´: {avg_time:.3f}ç§’/é¢˜")
        print(f"      æ€»æ—¶é—´: {elapsed_total:.1f}ç§’")
        import sys
        sys.stdout.flush()
        
        return result
    
    def run_full_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯æµ‹è¯•"""
        print(f"\nğŸ¯ å¼€å§‹ReasoningVå®Œæ•´éªŒè¯æµ‹è¯•")
        print(f"{'='*80}")
        
        self.load_model()
        
        results = {}
        total_questions = 0
        total_correct = 0
        
        # æŒ‰é¡ºåºæµ‹è¯•æ‰€æœ‰ä»»åŠ¡ï¼ˆFew-shotä»»åŠ¡è¿è¡Œå¤šæ¬¡å–å¹³å‡ï¼‰
        task_order = ["LDO Task", "Comparator Task", "Bandgap Task", 
                     "TQA Task", "Caption Task", "Opamp Task"]
        
        # Few-shotä»»åŠ¡éœ€è¦å¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼ˆä¸ä¼˜åŒ–æ—¶ä¸€è‡´ï¼‰
        few_shot_tasks = ["LDO Task", "Comparator Task", "Caption Task"]
        
        for task_name in task_order:
            if task_name in self.tasks:
                # Few-shotä»»åŠ¡è¿è¡Œ3æ¬¡å–å¹³å‡ï¼Œå…¶ä»–ä»»åŠ¡è¿è¡Œ1æ¬¡
                num_runs = 3 if task_name in few_shot_tasks else 1
                result = self.test_task(task_name, num_runs=num_runs)
                if result:
                    results[task_name] = result
                    total_questions += result['total_questions']
                    total_correct += result['correct_count']
        
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        overall_accuracy = total_correct / total_questions * 100 if total_questions > 0 else 0
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ å®Œæ•´éªŒè¯æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        print(f"\nğŸ“Š å„ä»»åŠ¡å‡†ç¡®ç‡:")
        for task_name, result in results.items():
            print(f"   {task_name}: {result['accuracy']:.2f}% ({result['correct_count']}/{result['total_questions']})")
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»é¢˜ç›®æ•°: {total_questions}")
        print(f"   æ€»æ­£ç¡®æ•°: {total_correct}")
        print(f"   æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.2f}%")
        
        return {
            'results': results,
            'overall_accuracy': overall_accuracy,
            'total_questions': total_questions,
            'total_correct': total_correct
        }
    
    def save_results(self, results: Dict[str, Any]):
        """ä¿å­˜ç»“æœ"""
        output = {
            'validation_results': results,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'model_path': self.model_path
        }
        
        with open("reasoningv_full_validation_results.json", 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: reasoningv_full_validation_results.json")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ReasoningVä¼˜åŒ–åå®Œæ•´éªŒè¯æµ‹è¯•å·¥å…·")
    print("ä½¿ç”¨æ‰€æœ‰ä¼˜åŒ–åçš„ç­–ç•¥é…ç½®ï¼Œå®Œæ•´æµ‹è¯•AMSBenchæ‰€æœ‰é¢˜ç›®")
    print("ç¡®ä¿å‡†ç¡®ç‡å¯é‡å¤")
    print("="*80)
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    reasoningv_path = "/home/ligengfei/LLM/Analogseeker-lgf/ReasoningV-7B"
    
    validator = ReasoningVFullValidation(reasoningv_path)
    results = validator.run_full_validation()
    
    if results:
        validator.save_results(results)
        
        # ä¸é¢„æœŸç»“æœå¯¹æ¯”
        expected_results = {
            "LDO Task": 80.67,
            "Comparator Task": 76.00,
            "Bandgap Task": 70.00,
            "TQA Task": 93.32,
            "Caption Task": 60.24,
            "Opamp Task": 58.33
        }
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ä¸é¢„æœŸç»“æœå¯¹æ¯”:")
        print(f"{'='*80}")
        all_match = True
        for task_name, result in results['results'].items():
            expected = expected_results.get(task_name, 0)
            actual = result['accuracy']
            diff = actual - expected
            match = abs(diff) < 0.5  # å…è®¸0.5%çš„è¯¯å·®
            status = "âœ…" if match else "âš ï¸"
            print(f"   {status} {task_name}:")
            print(f"      é¢„æœŸ: {expected:.2f}%, å®é™…: {actual:.2f}%, å·®å¼‚: {diff:+.2f}%")
            if not match:
                all_match = False
        
        if all_match:
            print(f"\nâœ… æ‰€æœ‰ä»»åŠ¡å‡†ç¡®ç‡ä¸é¢„æœŸä¸€è‡´ï¼ŒéªŒè¯é€šè¿‡ï¼")
        else:
            print(f"\nâš ï¸ éƒ¨åˆ†ä»»åŠ¡å‡†ç¡®ç‡ä¸é¢„æœŸå­˜åœ¨å·®å¼‚ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    return results

if __name__ == "__main__":
    main()

