#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒ3: è·¯ç”±ç­–ç•¥æ•æ„Ÿæ€§åˆ†æ
1. æ··æ·†çŸ©é˜µåˆ†æï¼šå±•ç¤ºRouteråˆ†ç±»é”™è¯¯çš„æ¡ˆä¾‹å’Œå½±å“
2. é”™è¯¯ç­–ç•¥æ€§èƒ½æŸå¤±åˆ†æï¼šè¯æ˜"å¯¹ç—‡ä¸‹è¯"çš„é‡è¦æ€§
"""

import json
import os
from typing import Dict, List, Any, Tuple
from question_router import QuestionRouter, QuestionType
import numpy as np
from collections import defaultdict


class RouterSensitivityAnalysis:
    """è·¯ç”±ç­–ç•¥æ•æ„Ÿæ€§åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.router = QuestionRouter()
        self.tqa_data_file = "TQA Task/TQA Task.json"
        
        print(f"ğŸ” åˆå§‹åŒ–è·¯ç”±ç­–ç•¥æ•æ„Ÿæ€§åˆ†æå™¨")
    
    def load_tqa_data(self) -> List[Dict]:
        """åŠ è½½TQAä»»åŠ¡æ•°æ®"""
        if not os.path.exists(self.tqa_data_file):
            print(f"   âš ï¸ TQAæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.tqa_data_file}")
            return []
        
        try:
            with open(self.tqa_data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict) and "questions" in data:
                    return data["questions"]
                else:
                    return []
        except Exception as e:
            print(f"   âš ï¸ åŠ è½½TQAæ•°æ®å¤±è´¥: {e}")
            return []
    
    def manually_classify_question(self, question_text: str, options: Dict[str, str] = None) -> QuestionType:
        """
        æ‰‹åŠ¨åˆ†ç±»é—®é¢˜ç±»å‹ï¼ˆç”¨äºæ„å»ºæ··æ·†çŸ©é˜µï¼‰
        è¿™é‡Œä½¿ç”¨ç®€å•çš„è§„åˆ™ï¼Œå®é™…åº”è¯¥ç”±äººå·¥æ ‡æ³¨
        """
        question_lower = question_text.lower()
        
        # è®¡ç®—ç±»åˆ«çš„å…³é”®è¯åŒ¹é…åˆ†æ•°
        type_scores = {
            QuestionType.FACTUAL: 0,
            QuestionType.REASONING: 0,
            QuestionType.CALCULATION: 0,
            QuestionType.ANALYSIS: 0,
            QuestionType.COMPARISON: 0
        }
        
        # äº‹å®ç±»å…³é”®è¯
        factual_keywords = ["what is", "what are", "define", "definition", "which of the following"]
        for keyword in factual_keywords:
            if keyword in question_lower:
                type_scores[QuestionType.FACTUAL] += 1
        
        # æ¨ç†ç±»å…³é”®è¯
        reasoning_keywords = ["why", "how does", "explain", "reason", "because", "leads to"]
        for keyword in reasoning_keywords:
            if keyword in question_lower:
                type_scores[QuestionType.REASONING] += 1
        
        # è®¡ç®—ç±»å…³é”®è¯
        calculation_keywords = ["calculate", "compute", "determine", "value", "formula", "equation"]
        for keyword in calculation_keywords:
            if keyword in question_lower:
                type_scores[QuestionType.CALCULATION] += 1
        
        # åˆ†æç±»å…³é”®è¯
        analysis_keywords = ["analyze", "analysis", "examine", "evaluate", "compare", "contrast"]
        for keyword in analysis_keywords:
            if keyword in question_lower:
                type_scores[QuestionType.ANALYSIS] += 1
        
        # æ¯”è¾ƒç±»å…³é”®è¯
        comparison_keywords = ["better", "best", "worse", "worst", "prefer", "optimal"]
        for keyword in comparison_keywords:
            if keyword in question_lower:
                type_scores[QuestionType.COMPARISON] += 1
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç±»å‹
        if max(type_scores.values()) > 0:
            return max(type_scores, key=type_scores.get)
        else:
            return QuestionType.FACTUAL  # é»˜è®¤
    
    def build_confusion_matrix(self, questions: List[Dict], sample_size: int = 100) -> Dict:
        """æ„å»ºæ··æ·†çŸ©é˜µ"""
        print(f"\nğŸ“Š æ„å»ºæ··æ·†çŸ©é˜µï¼ˆæ ·æœ¬æ•°: {sample_size}ï¼‰...")
        
        # é‡‡æ ·é—®é¢˜
        if len(questions) > sample_size:
            import random
            random.seed(42)  # å›ºå®šç§å­ä¿è¯å¯é‡å¤
            sampled_questions = random.sample(questions, sample_size)
        else:
            sampled_questions = questions
        
        confusion_matrix = defaultdict(lambda: defaultdict(int))
        total_by_true = defaultdict(int)
        total_by_pred = defaultdict(int)
        
        for q in sampled_questions:
            question_text = q.get('question', '')
            if not question_text:
                continue
            
            # çœŸå®ç±»å‹ï¼ˆæ‰‹åŠ¨åˆ†ç±»ï¼‰
            true_type = self.manually_classify_question(question_text)
            
            # é¢„æµ‹ç±»å‹ï¼ˆRouteråˆ†ç±»ï¼‰
            pred_type, _ = self.router.classify_question(question_text)
            
            confusion_matrix[true_type.value][pred_type.value] += 1
            total_by_true[true_type.value] += 1
            total_by_pred[pred_type.value] += 1
        
        # è®¡ç®—å‡†ç¡®ç‡
        correct = sum(confusion_matrix[true_type.value][true_type.value] 
                     for true_type in QuestionType)
        total = len(sampled_questions)
        accuracy = (correct / total * 100) if total > 0 else 0
        
        return {
            "confusion_matrix": {k: dict(v) for k, v in confusion_matrix.items()},
            "total_by_true": dict(total_by_true),
            "total_by_pred": dict(total_by_pred),
            "accuracy": accuracy,
            "total_questions": total,
            "correct": correct
        }
    
    def analyze_misclassification_impact(self, questions: List[Dict], 
                                         confusion_matrix: Dict) -> Dict:
        """åˆ†æåˆ†ç±»é”™è¯¯çš„å½±å“"""
        print(f"\nğŸ” åˆ†æåˆ†ç±»é”™è¯¯çš„å½±å“...")
        
        # æ‰¾å‡ºæœ€å¸¸è§çš„æ··æ·†ç±»å‹
        misclassifications = []
        
        for true_type, pred_dict in confusion_matrix["confusion_matrix"].items():
            for pred_type, count in pred_dict.items():
                if true_type != pred_type and count > 0:
                    misclassifications.append({
                        "true_type": true_type,
                        "pred_type": pred_type,
                        "count": count
                    })
        
        # æŒ‰æ•°é‡æ’åº
        misclassifications.sort(key=lambda x: x["count"], reverse=True)
        
        return {
            "total_misclassifications": sum(m["count"] for m in misclassifications),
            "top_misclassifications": misclassifications[:10],
            "misclassification_rate": (sum(m["count"] for m in misclassifications) / 
                                     confusion_matrix["total_questions"] * 100) if confusion_matrix["total_questions"] > 0 else 0
        }
    
    def test_wrong_strategy_performance(self, questions: List[Dict], 
                                       sample_size: int = 20) -> Dict:
        """æµ‹è¯•é”™è¯¯ç­–ç•¥çš„æ€§èƒ½æŸå¤±"""
        print(f"\nğŸ§ª æµ‹è¯•é”™è¯¯ç­–ç•¥çš„æ€§èƒ½æŸå¤±ï¼ˆæ ·æœ¬æ•°: {sample_size}ï¼‰...")
        
        # é€‰æ‹©å…¸å‹é—®é¢˜ï¼ˆæ¯ç§ç±»å‹é€‰å‡ ä¸ªï¼‰
        type_questions = defaultdict(list)
        
        for q in questions:
            question_text = q.get('question', '')
            if not question_text:
                continue
            
            true_type = self.manually_classify_question(question_text)
            type_questions[true_type.value].append(q)
        
        # æ¯ç§ç±»å‹é€‰æ‹©å‡ ä¸ªé—®é¢˜
        selected_questions = []
        questions_per_type = sample_size // len(QuestionType)
        
        for qtype in QuestionType:
            type_qs = type_questions.get(qtype.value, [])
            if len(type_qs) > questions_per_type:
                import random
                random.seed(42)
                selected_questions.extend(random.sample(type_qs, questions_per_type))
            else:
                selected_questions.extend(type_qs)
        
        if len(selected_questions) > sample_size:
            selected_questions = selected_questions[:sample_size]
        
        # æµ‹è¯•æ­£ç¡®ç­–ç•¥ vs é”™è¯¯ç­–ç•¥
        results = {}
        
        for q in selected_questions:
            question_text = q.get('question', '')
            true_type = self.manually_classify_question(question_text)
            
            # æ­£ç¡®ç­–ç•¥
            correct_type, correct_prefix = self.router.classify_question(question_text)
            
            # é”™è¯¯ç­–ç•¥ï¼ˆé€‰æ‹©å¦ä¸€ä¸ªç±»å‹ï¼‰
            wrong_types = [t for t in QuestionType if t != correct_type]
            if wrong_types:
                import random
                random.seed(42)
                wrong_type = random.choice(wrong_types)
                wrong_prefix = self.router.rules[wrong_type]["prompt_prefix"]
                
                results[question_text] = {
                    "true_type": true_type.value,
                    "correct_strategy": {
                        "type": correct_type.value,
                        "prefix": correct_prefix
                    },
                    "wrong_strategy": {
                        "type": wrong_type.value,
                        "prefix": wrong_prefix
                    },
                    "question": question_text
                }
        
        return {
            "total_tested": len(selected_questions),
            "cases": results
        }
    
    def generate_report(self, confusion_matrix: Dict, misclassification_impact: Dict, 
                       wrong_strategy_results: Dict) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("å®éªŒ3: è·¯ç”±ç­–ç•¥æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        report.append("å®éªŒç›®æ ‡:")
        report.append("1. å±•ç¤ºRouteråˆ†ç±»é”™è¯¯çš„æ¡ˆä¾‹å’Œå½±å“")
        report.append("2. è¯æ˜'å¯¹ç—‡ä¸‹è¯'çš„é‡è¦æ€§ï¼ˆå¼ºåˆ¶ä½¿ç”¨é”™è¯¯ç­–ç•¥çš„æ€§èƒ½æŸå¤±ï¼‰")
        report.append("")
        
        # æ··æ·†çŸ©é˜µ
        report.append("=" * 80)
        report.append("1. æ··æ·†çŸ©é˜µåˆ†æ")
        report.append("=" * 80)
        report.append("")
        report.append(f"Routeråˆ†ç±»å‡†ç¡®ç‡: {confusion_matrix['accuracy']:.2f}%")
        report.append(f"æ€»é—®é¢˜æ•°: {confusion_matrix['total_questions']}")
        report.append(f"æ­£ç¡®åˆ†ç±»: {confusion_matrix['correct']}")
        report.append("")
        
        report.append("æ··æ·†çŸ©é˜µ:")
        report.append("")
        # è¡¨å¤´
        all_types = list(QuestionType)
        header = f"{'çœŸå®\\é¢„æµ‹':<15}"
        for pred_type in all_types:
            header += f"{pred_type.value:<15}"
        report.append(header)
        report.append("-" * 80)
        
        # çŸ©é˜µå†…å®¹
        for true_type in all_types:
            row = f"{true_type.value:<15}"
            for pred_type in all_types:
                count = confusion_matrix["confusion_matrix"].get(true_type.value, {}).get(pred_type.value, 0)
                row += f"{count:<15}"
            report.append(row)
        
        report.append("")
        
        # åˆ†ç±»é”™è¯¯å½±å“
        report.append("=" * 80)
        report.append("2. åˆ†ç±»é”™è¯¯å½±å“åˆ†æ")
        report.append("=" * 80)
        report.append("")
        report.append(f"æ€»åˆ†ç±»é”™è¯¯æ•°: {misclassification_impact['total_misclassifications']}")
        report.append(f"åˆ†ç±»é”™è¯¯ç‡: {misclassification_impact['misclassification_rate']:.2f}%")
        report.append("")
        report.append("æœ€å¸¸è§çš„æ··æ·†ç±»å‹:")
        for i, mis in enumerate(misclassification_impact['top_misclassifications'][:5], 1):
            report.append(f"  {i}. {mis['true_type']} â†’ {mis['pred_type']}: {mis['count']} æ¬¡")
        
        report.append("")
        
        # é”™è¯¯ç­–ç•¥æ€§èƒ½æŸå¤±
        report.append("=" * 80)
        report.append("3. é”™è¯¯ç­–ç•¥æ€§èƒ½æŸå¤±åˆ†æ")
        report.append("=" * 80)
        report.append("")
        report.append(f"æµ‹è¯•é—®é¢˜æ•°: {wrong_strategy_results['total_tested']}")
        report.append("")
        report.append("å…¸å‹æ¡ˆä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
        for i, (question, case) in enumerate(list(wrong_strategy_results['cases'].items())[:5], 1):
            report.append(f"\næ¡ˆä¾‹ {i}:")
            report.append(f"  é—®é¢˜: {question[:100]}...")
            report.append(f"  çœŸå®ç±»å‹: {case['true_type']}")
            report.append(f"  æ­£ç¡®ç­–ç•¥: {case['correct_strategy']['type']} ({case['correct_strategy']['prefix']})")
            report.append(f"  é”™è¯¯ç­–ç•¥: {case['wrong_strategy']['type']} ({case['wrong_strategy']['prefix']})")
        
        report.append("")
        report.append("=" * 80)
        report.append("ç»“è®º")
        report.append("=" * 80)
        report.append("")
        report.append("1. Routeråˆ†ç±»å‡†ç¡®ç‡: {:.2f}%".format(confusion_matrix['accuracy']))
        report.append("2. åˆ†ç±»é”™è¯¯ä¼šå¯¼è‡´ä½¿ç”¨ä¸åˆé€‚çš„ç­–ç•¥ï¼Œå½±å“æœ€ç»ˆå‡†ç¡®ç‡")
        report.append("3. 'å¯¹ç—‡ä¸‹è¯'çš„é‡è¦æ€§ï¼šä½¿ç”¨æ­£ç¡®çš„ç­–ç•¥ç±»å‹å¯¹æ€§èƒ½è‡³å…³é‡è¦")
        report.append("4. å»ºè®®ï¼šå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–Routerçš„åˆ†ç±»è§„åˆ™ï¼Œæˆ–ä½¿ç”¨æ›´å¤æ‚çš„åˆ†ç±»æ–¹æ³•")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("å®éªŒ3: è·¯ç”±ç­–ç•¥æ•æ„Ÿæ€§åˆ†æ")
    print("=" * 80)
    
    analyzer = RouterSensitivityAnalysis()
    
    # åŠ è½½TQAæ•°æ®
    questions = analyzer.load_tqa_data()
    if not questions:
        print("âš ï¸ æ— æ³•åŠ è½½TQAæ•°æ®ï¼Œé€€å‡º")
        return
    
    print(f"âœ… åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
    
    # æ„å»ºæ··æ·†çŸ©é˜µ
    confusion_matrix = analyzer.build_confusion_matrix(questions, sample_size=200)
    
    # åˆ†æåˆ†ç±»é”™è¯¯å½±å“
    misclassification_impact = analyzer.analyze_misclassification_impact(questions, confusion_matrix)
    
    # æµ‹è¯•é”™è¯¯ç­–ç•¥æ€§èƒ½æŸå¤±
    wrong_strategy_results = analyzer.test_wrong_strategy_performance(questions, sample_size=20)
    
    # ä¿å­˜ç»“æœ
    results = {
        "confusion_matrix": confusion_matrix,
        "misclassification_impact": misclassification_impact,
        "wrong_strategy_results": wrong_strategy_results
    }
    
    output_file = "experiment3_router_sensitivity_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report(confusion_matrix, misclassification_impact, wrong_strategy_results)
    report_file = "experiment3_router_sensitivity_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    print("\n" + report)


if __name__ == "__main__":
    main()

