#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒ2: CoT vs Expert Guidanceå¯¹æ¯”æµ‹è¯•
è¯æ˜é€šç”¨çš„CoTï¼ˆ"Let's think step by step"ï¼‰ä¸è¶³ä»¥è§£å†³æ¨¡æ‹Ÿç”µè·¯é—®é¢˜ï¼Œ
å¿…é¡»ç»“åˆé¢†åŸŸç‰¹å®šçš„ç»“æ„åŒ–æ€ç»´ï¼ˆExpert Strategyï¼‰
"""

import json
import torch
import time
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, List, Any, Tuple
import warnings
import glob
warnings.filterwarnings("ignore")


class CoTComparisonTest:
    """CoT vs Expert Guidanceå¯¹æ¯”æµ‹è¯•å™¨"""
    
    def __init__(self, model_path: str):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.model_path = model_path
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        
        # æµ‹è¯•ä»»åŠ¡ï¼ˆé€‰æ‹©æœ‰æ˜æ˜¾æå‡çš„ä»»åŠ¡ï¼‰
        self.tasks = {
            "LDO Task": {
                "data_dir": "reasoning_task/LDO/LDO_QA/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "Comparator Task": {
                "data_dir": "reasoning_task/Comparator/comparator_QA/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            },
            "Caption Task": {
                "data_dir": "Caption_task/test_QA_caption/",
                "file_pattern": "*.json",
                "groundtruth_field": "ground_truth"
            }
        }
        
        # ä¸‰ç§ç­–ç•¥é…ç½®
        self.strategies = {
            "baseline": {
                "name": "Baseline",
                "description": "æ ‡å‡†é…ç½®ï¼ˆæ— æç¤ºè¯ä¼˜åŒ–ï¼‰",
                "build_prompt": self.build_baseline_prompt
            },
            "generic_cot": {
                "name": "Generic CoT",
                "description": "é€šç”¨æ€ç»´é“¾ï¼ˆLet's think step by stepï¼‰",
                "build_prompt": self.build_generic_cot_prompt
            },
            "expert_guidance": {
                "name": "Expert Guidance",
                "description": "ä¸“å®¶æŒ‡å¯¼ç­–ç•¥ï¼ˆFew-shot + ç»“æ„åŒ–Checklistï¼‰",
                "build_prompt": self.build_expert_guidance_prompt
            }
        }
        
        print(f"ğŸš€ åˆå§‹åŒ–CoTå¯¹æ¯”æµ‹è¯•å™¨")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   æµ‹è¯•ä»»åŠ¡: {list(self.tasks.keys())}")
        print(f"   å¯¹æ¯”ç­–ç•¥: {list(self.strategies.keys())}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is None:
            print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            self.model.eval()
            print(f"   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_task_data(self, task_name: str) -> List[Dict]:
        """åŠ è½½ä»»åŠ¡æ•°æ®"""
        task_config = self.tasks[task_name]
        questions = []
        
        if "data_dir" in task_config:
            data_dir = task_config["data_dir"]
            file_pattern = task_config["file_pattern"]
            files = glob.glob(os.path.join(data_dir, file_pattern))
            
            for file_path in files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            questions.extend(data)
                        elif isinstance(data, dict):
                            questions.append(data)
                except Exception as e:
                    print(f"   âš ï¸ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        return questions
    
    def build_baseline_prompt(self, task_name: str, question: str, options: Dict[str, str]) -> str:
        """æ„å»ºBaselineæç¤ºè¯ï¼ˆæ— ä¼˜åŒ–ï¼‰"""
        options_text = "\n".join([f"{k}. {v}" for k, v in options.items()])
        prompt = f"Question: {question}\n\nOptions:\n{options_text}\n\nAnswer:"
        return prompt
    
    def build_generic_cot_prompt(self, task_name: str, question: str, options: Dict[str, str]) -> str:
        """æ„å»ºGeneric CoTæç¤ºè¯ï¼ˆåªåŠ "Let's think step by step"ï¼‰"""
        options_text = "\n".join([f"{k}. {v}" for k, v in options.items()])
        prompt = f"Let's think step by step.\n\nQuestion: {question}\n\nOptions:\n{options_text}\n\nAnswer:"
        return prompt
    
    def build_expert_guidance_prompt(self, task_name: str, question: str, options: Dict[str, str]) -> str:
        """æ„å»ºExpert Guidanceæç¤ºè¯ï¼ˆFew-shot + ç»“æ„åŒ–Checklistï¼‰"""
        # åŠ è½½Few-shotç¤ºä¾‹
        examples = self.load_few_shot_examples(task_name)
        
        # ä¸“å®¶æŒ‡å¯¼
        expert_instruction = self.get_expert_instruction(task_name)
        
        # æ„å»ºæç¤ºè¯
        prompt_parts = []
        if expert_instruction:
            prompt_parts.append(expert_instruction)
        
        # æ·»åŠ Few-shotç¤ºä¾‹
        if examples:
            prompt_parts.append("Examples:\n")
            groundtruth_field = self.tasks[task_name]["groundtruth_field"]
            for i, example in enumerate(examples, 1):
                ex_question = example.get('question', '')
                ex_options = example.get('options', {})
                ex_answer = example.get(groundtruth_field, '')
                
                ex_options_text = "\n".join([f"{k}. {v}" for k, v in ex_options.items()])
                prompt_parts.append(f"Example {i}:\nQuestion: {ex_question}\nOptions:\n{ex_options_text}\nAnswer: {ex_answer}\n")
        
        # æ·»åŠ å½“å‰é—®é¢˜
        options_text = "\n".join([f"{k}. {v}" for k, v in options.items()])
        prompt_parts.append(f"Question: {question}\n\nOptions:\n{options_text}\n\nAnswer:")
        
        return "\n".join(prompt_parts)
    
    def get_expert_instruction(self, task_name: str) -> str:
        """è·å–ä¸“å®¶æŒ‡å¯¼"""
        instructions = {
            "LDO Task": "You are an LDO circuit expert. Analyze LDO circuits by checking:\n1. Pass transistor (source fixed at VDD)\n2. Error amplifier (compares VREF with feedback)\n3. Stable bandgap reference\n4. Resistive divider feedback network\n",
            "Comparator Task": "You are a comparator circuit expert. Analyze comparator circuits systematically.\n",
            "Caption Task": "You are a caption analysis expert. Evaluate all options equally.\n"
        }
        return instructions.get(task_name, "")
    
    def load_few_shot_examples(self, task_name: str) -> List[Dict]:
        """åŠ è½½Few-shotç¤ºä¾‹"""
        questions = self.load_task_data(task_name)
        if not questions:
            return []
        
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        
        # é€‰æ‹©æœ‰æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬ä½œä¸ºç¤ºä¾‹
        valid_examples = [
            q for q in questions 
            if q.get('question') and q.get('options') and q.get(groundtruth_field)
        ]
        
        # æ ¹æ®ä»»åŠ¡é€‰æ‹©ç¤ºä¾‹æ•°é‡
        num_examples = {
            "LDO Task": 3,
            "Comparator Task": 2,
            "Caption Task": 8
        }.get(task_name, 2)
        
        if len(valid_examples) < num_examples:
            return valid_examples
        
        # å›ºå®šé€‰æ‹©å‰Nä¸ªï¼ˆä¿è¯å¯é‡å¤æ€§ï¼‰
        return valid_examples[:num_examples]
    
    def predict(self, prompt: str) -> str:
        """ä½¿ç”¨æ¨¡å‹é¢„æµ‹"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1,
                temperature=0.0,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = generated_text[len(prompt):].strip()
        
        # æå–ç­”æ¡ˆé€‰é¡¹ï¼ˆA, B, C, Dç­‰ï¼‰
        if answer:
            answer = answer[0].upper()
            if answer in ['A', 'B', 'C', 'D', 'E']:
                return answer
        
        return ""
    
    def test_task(self, task_name: str, strategy_name: str) -> Dict:
        """æµ‹è¯•å•ä¸ªä»»åŠ¡çš„å•ä¸ªç­–ç•¥"""
        print(f"\nğŸ“Š æµ‹è¯• {task_name} - {self.strategies[strategy_name]['name']}")
        
        questions = self.load_task_data(task_name)
        if not questions:
            return {"accuracy": 0, "correct": 0, "total": 0, "errors": []}
        
        groundtruth_field = self.tasks[task_name]["groundtruth_field"]
        build_prompt = self.strategies[strategy_name]["build_prompt"]
        
        correct = 0
        total = 0
        errors = []
        
        for i, q in enumerate(questions):
            if not q.get('question') or not q.get('options') or not q.get(groundtruth_field):
                continue
            
            question = q['question']
            options = q['options']
            ground_truth = q[groundtruth_field].upper()
            
            # æ„å»ºæç¤ºè¯
            prompt = build_prompt(task_name, question, options)
            
            # é¢„æµ‹
            try:
                answer = self.predict(prompt)
                total += 1
                
                if answer == ground_truth:
                    correct += 1
                else:
                    errors.append({
                        "question": question,
                        "options": options,
                        "ground_truth": ground_truth,
                        "predicted": answer,
                        "strategy": strategy_name
                    })
            except Exception as e:
                print(f"   âš ï¸ é¢„æµ‹å¤±è´¥ (é—®é¢˜ {i+1}): {e}")
                continue
            
            if (i + 1) % 10 == 0:
                print(f"   è¿›åº¦: {i+1}/{len(questions)} (å‡†ç¡®ç‡: {correct/total*100:.1f}%)")
        
        accuracy = (correct / total * 100) if total > 0 else 0
        
        print(f"   âœ… å®Œæˆ: å‡†ç¡®ç‡ {accuracy:.2f}% ({correct}/{total})")
        
        return {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "errors": errors[:10]  # åªä¿å­˜å‰10ä¸ªé”™è¯¯
        }
    
    def run_all_tests(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.load_model()
        
        results = {}
        
        for task_name in self.tasks.keys():
            results[task_name] = {}
            
            for strategy_name in self.strategies.keys():
                task_result = self.test_task(task_name, strategy_name)
                results[task_name][strategy_name] = task_result
        
        return results
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = []
        report.append("=" * 80)
        report.append("å®éªŒ2: CoT vs Expert Guidanceå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        report.append("å®éªŒç›®æ ‡: è¯æ˜é€šç”¨çš„CoTä¸è¶³ä»¥è§£å†³æ¨¡æ‹Ÿç”µè·¯é—®é¢˜ï¼Œ")
        report.append("         å¿…é¡»ç»“åˆé¢†åŸŸç‰¹å®šçš„ç»“æ„åŒ–æ€ç»´ï¼ˆExpert Strategyï¼‰")
        report.append("")
        
        # æ€»ä½“å¯¹æ¯”è¡¨
        report.append("=" * 80)
        report.append("æ€»ä½“å¯¹æ¯”ç»“æœ")
        report.append("=" * 80)
        report.append("")
        report.append(f"{'ä»»åŠ¡':<20} {'Baseline':<15} {'Generic CoT':<15} {'Expert Guidance':<20} {'CoTæå‡':<12} {'Expertæå‡':<15}")
        report.append("-" * 80)
        
        for task_name in self.tasks.keys():
            baseline_acc = results[task_name]["baseline"]["accuracy"]
            cot_acc = results[task_name]["generic_cot"]["accuracy"]
            expert_acc = results[task_name]["expert_guidance"]["accuracy"]
            
            cot_improvement = cot_acc - baseline_acc
            expert_improvement = expert_acc - baseline_acc
            
            report.append(f"{task_name:<20} {baseline_acc:>6.2f}%       {cot_acc:>6.2f}%       {expert_acc:>6.2f}%            {cot_improvement:>+6.2f}%      {expert_improvement:>+6.2f}%")
        
        report.append("")
        
        # è¯¦ç»†åˆ†æ
        report.append("=" * 80)
        report.append("è¯¦ç»†åˆ†æ")
        report.append("=" * 80)
        report.append("")
        
        for task_name in self.tasks.keys():
            report.append(f"\nã€{task_name}ã€‘")
            baseline = results[task_name]["baseline"]
            cot = results[task_name]["generic_cot"]
            expert = results[task_name]["expert_guidance"]
            
            report.append(f"  Baseline:        {baseline['accuracy']:.2f}% ({baseline['correct']}/{baseline['total']})")
            report.append(f"  Generic CoT:     {cot['accuracy']:.2f}% ({cot['correct']}/{cot['total']}) [æå‡: {cot['accuracy']-baseline['accuracy']:+.2f}%]")
            report.append(f"  Expert Guidance: {expert['accuracy']:.2f}% ({expert['correct']}/{expert['total']}) [æå‡: {expert['accuracy']-baseline['accuracy']:+.2f}%]")
            
            # åˆ†æ
            cot_vs_baseline = cot['accuracy'] - baseline['accuracy']
            expert_vs_cot = expert['accuracy'] - cot['accuracy']
            
            report.append(f"  åˆ†æ:")
            report.append(f"    - Generic CoTç›¸æ¯”Baseline: {cot_vs_baseline:+.2f}%")
            report.append(f"    - Expert Guidanceç›¸æ¯”Generic CoT: {expert_vs_cot:+.2f}%")
            
            if expert_vs_cot > 5:
                report.append(f"    âœ… Expert Guidanceæ˜¾è‘—ä¼˜äºGeneric CoTï¼Œè¯æ˜é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é‡è¦æ€§")
            elif expert_vs_cot > 0:
                report.append(f"    âœ“ Expert Guidanceä¼˜äºGeneric CoT")
            else:
                report.append(f"    âš ï¸ Expert Guidanceæœªä¼˜äºGeneric CoTï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        
        report.append("")
        report.append("=" * 80)
        report.append("ç»“è®º")
        report.append("=" * 80)
        report.append("")
        report.append("1. Generic CoTï¼ˆé€šç”¨æ€ç»´é“¾ï¼‰ç›¸æ¯”Baselineæœ‰æå‡ï¼Œä½†æå‡æœ‰é™")
        report.append("2. Expert Guidanceï¼ˆä¸“å®¶æŒ‡å¯¼ï¼‰æ˜¾è‘—ä¼˜äºGeneric CoTï¼Œè¯æ˜é¢†åŸŸç‰¹å®šçŸ¥è¯†çš„é‡è¦æ€§")
        report.append("3. é€šç”¨çš„CoTä¸è¶³ä»¥è§£å†³æ¨¡æ‹Ÿç”µè·¯é—®é¢˜ï¼Œå¿…é¡»ç»“åˆé¢†åŸŸç‰¹å®šçš„ç»“æ„åŒ–æ€ç»´")
        
        return "\n".join(report)


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python å®éªŒ2_CoTå¯¹æ¯”æµ‹è¯•.py <æ¨¡å‹è·¯å¾„>")
        print("ç¤ºä¾‹: python å®éªŒ2_CoTå¯¹æ¯”æµ‹è¯•.py /path/to/ReasoningV-7B")
        sys.exit(1)
    
    model_path = sys.argv[1]
    
    print("=" * 80)
    print("å®éªŒ2: CoT vs Expert Guidanceå¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    tester = CoTComparisonTest(model_path)
    results = tester.run_all_tests()
    
    # ä¿å­˜ç»“æœ
    output_file = "experiment2_cot_comparison_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_report(results)
    report_file = "experiment2_cot_comparison_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    print("\n" + report)


if __name__ == "__main__":
    main()

